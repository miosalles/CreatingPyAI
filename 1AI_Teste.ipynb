{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMX0u05EFIdN2d2uR8lUCYQ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Instalando o SDK do Google.\n",
        "\n"
      ],
      "metadata": {
        "id": "I5UYRT6CXSx5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5DNtR4nWTVG"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the Python SDK\n",
        "import google.generativeai as genai\n",
        "\n",
        "GOOGLE_API_KEY='AIzaSyAQLC_wgmJzmlctpl3Lp0r6AB3Ha3MudcY'\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "eWd7u1adW999"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*  Listar os modelos disponíveis.\n",
        "\n"
      ],
      "metadata": {
        "id": "qJ_jixROXejX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for m in genai.list_models():\n",
        "  if 'generateContent' in m.supported_generation_methods:\n",
        "    print(m.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "id": "E1i-dPBiXJno",
        "outputId": "51341777-d3cd-4028-be01-9870fbdfb7d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/gemini-1.0-pro\n",
            "models/gemini-1.0-pro-001\n",
            "models/gemini-1.0-pro-latest\n",
            "models/gemini-1.0-pro-vision-latest\n",
            "models/gemini-1.5-pro-latest\n",
            "models/gemini-pro\n",
            "models/gemini-pro-vision\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Temperature vai de 0 a 1, quanto maior, mais criativo o chatbot, porém correndo um pouco mais risco de respostas menos realistas."
      ],
      "metadata": {
        "id": "29jUa5eHcvbl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generation_config = {\n",
        "    'candidate_count': 1,\n",
        "    'temperature': 0.5,\n",
        "}"
      ],
      "metadata": {
        "id": "Tkwhis9SaHeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configurações de segurança, nesse caso a segurança está nula para todos os conteúdos abaixo, para manter padrão, em filtrar \"alguns\" conteúdos, essa etapa poderia ser pulada e ele deixaria a configuração padrão do Google que é média."
      ],
      "metadata": {
        "id": "dTWSIyXwc5rg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "safety_settings = {\n",
        "    'HARASSMENT':'BLOCK_NONE',\n",
        "    'SEXUAL': 'BLOCK_NONE',\n",
        "    'HATE': 'BLOCK_NONE',\n",
        "    'DANGEROUS': 'BLOCK_NONE'\n",
        "}"
      ],
      "metadata": {
        "id": "Ea6SpE_Da48c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Inicializando o modelo:"
      ],
      "metadata": {
        "id": "oXpSOjU5dTsQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel(model_name = 'gemini-1.0-pro',\n",
        "                              generation_config=generation_config,\n",
        "                              safety_settings=safety_settings)"
      ],
      "metadata": {
        "id": "_VSlx9Xqboa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configurando o chat para respostas:"
      ],
      "metadata": {
        "id": "I6EFEWBlfW6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content('Liste cachorros de pelo curtos ou hipoalergênicos de porte pequeno que são permitidos e disponíveis no Brasil.')\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "JZDULRRueHY4",
        "outputId": "ac915941-4818-414d-b94e-cd5dedbc10ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Cachorros de Pelo Curto de Porte Pequeno**\n",
            "\n",
            "* Beagle\n",
            "* Boston Terrier\n",
            "* Bulldog Francês\n",
            "* Cavalier King Charles Spaniel\n",
            "* Chihuahua\n",
            "* Fox Terrier\n",
            "* Jack Russell Terrier\n",
            "* Maltês\n",
            "* Pug\n",
            "* Schnauzer Miniatura\n",
            "* Yorkshire Terrier\n",
            "\n",
            "**Cachorros Hipoalergênicos de Porte Pequeno**\n",
            "\n",
            "* Bichon Frise\n",
            "* Caniche Toy\n",
            "* Havanese\n",
            "* Lhasa Apso\n",
            "* Maltês\n",
            "* Poodle Toy\n",
            "* Schnauzer Miniatura\n",
            "* Shih Tzu\n",
            "* Terrier Escocês\n",
            "* West Highland White Terrier\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat = model.start_chat(history=[])"
      ],
      "metadata": {
        "id": "i6Bu1D13g0Hi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " * Configurando o chat para que o usuário digite a pergunta/prompt e o bot busque a resposta.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Fe74vM-BiZs_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = input('Como posso te ajudar hoje? ')\n",
        "\n",
        "while prompt != 'fim':\n",
        "  response = chat.send_message(prompt)\n",
        "  print('Resposta:', response.text, '\\n')\n",
        "  prompt = input('Como posso te ajudar hoje?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "4PvbnjJGhd-x",
        "outputId": "1f0c922e-fd47-41c1-8410-d7bef2ee5385"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Como posso te ajudar hoje? \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "content must not be empty",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-5909f7cd585f>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'fim'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Resposta:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Como posso te ajudar hoje?'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/generativeai/generative_models.py\u001b[0m in \u001b[0;36msend_message\u001b[0;34m(self, content, generation_config, safety_settings, stream, tools, tool_config)\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0mtools_lib\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tools_lib\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m         \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontent_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrole\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/generativeai/types/content_types.py\u001b[0m in \u001b[0;36mto_content\u001b[0;34m(content)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mto_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mContentType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"content must not be empty\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: content must not be empty"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "72Uqrearmlp0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}